# Task ID: 6
# Title: Fix and Enhance Test Infrastructure
# Status: pending
# Dependencies: 1, 3
# Priority: medium
# Description: Overhaul the project's testing infrastructure by resolving dependency issues, creating a stable test environment, fixing broken tests, and implementing a CI pipeline, performance benchmarking, and WebSocket mocking.
# Details:
1. **Dependency Management Consolidation:**
   - Identify and add the missing `packaging` module to the project's core dependencies.
   - Audit all existing code (including from Tasks 1 & 3) to identify all required libraries (e.g., `ccxt`, `ccxt.pro`, `textual`, `pytest`, `pytest-benchmark`, `pytest-asyncio`).
   - Consolidate all project and development dependencies into `pyproject.toml` under `[project.dependencies]` and `[project.optional-dependencies]`, deprecating any `requirements.txt` files.

2. **Test Environment Setup:**
   - Create a `Makefile` or a shell script (`scripts/setup-dev.sh`) that automates the creation of a virtual environment and installation of all project and test dependencies using `pip install -e .[test]`.
   - Ensure the script is executable and documented in the project's `README.md`.

3. **Test Codebase Refactoring:**
   - Systematically review all files under the `tests/` directory.
   - Correct all broken `import` statements and file paths that resulted from recent refactoring. Ensure all tests can be discovered and run by `pytest` from the project root.

4. **WebSocket Mocking Framework:**
   - Create a new mock server/client infrastructure in `tests/mocks/websocket.py`.
   - The mock must be able to simulate the lifecycle of a WebSocket connection as defined in Task 3: successful connection, authentication, subscription, data message pushes (e.g., mock trades, order book updates), heartbeats, and graceful/abrupt disconnections.
   - It should be easily configurable within `pytest` fixtures to simulate various scenarios, including high-volume data and error conditions.

5. **Performance Benchmarking Suite:**
   - Integrate the `pytest-benchmark` library.
   - Create a new test suite `tests/performance/` for benchmarking critical code paths.
   - Initial benchmarks should target data transformation logic in adapters and event processing throughput in the `EventEngine`.

6. **Continuous Integration (CI) Pipeline:**
   - Set up a CI workflow using GitHub Actions (in `.github/workflows/ci.yml`).
   - The pipeline should trigger on every push and pull request to the `main` branch.
   - Workflow steps must include:
     - Checking out the code.
     - Setting up a specific Python version.
     - Installing dependencies using the new setup script.
     - Running static analysis/linting tools (e.g., `flake8`, `black`).
     - Executing the full `pytest` suite.

# Test Strategy:
1. **Environment Script Verification:**
   - In a clean, containerized environment (e.g., Docker), execute the new setup script. Verify that the virtual environment is created and all dependencies from `pyproject.toml` are installed correctly.
   - Confirm that `pytest` can be run successfully from the command line after setup.

2. **CI Pipeline Validation:**
   - Create a test branch and a pull request.
   - Confirm that the CI pipeline triggers automatically.
   - Intentionally introduce a failing test and a linting error to ensure the respective CI steps fail as expected.
   - Fix the errors and confirm the pipeline passes successfully.

3. **Mock Infrastructure Test:**
   - Write a new unit test for a component that will use the WebSocket streaming (e.g., a test for the `StreamingBaseAdapter`).
   - Use the new WebSocket mock fixture to simulate a data stream and assert that the component processes the mock data correctly. This validates the usability of the mock framework.

4. **Benchmark Execution:**
   - Run the performance test suite locally using `pytest --benchmark-only`.
   - Verify that benchmark results are generated and saved. Review the initial report to establish a performance baseline.

5. **Full Test Suite Execution:**
   - Run the entire test suite (`pytest`) from the project root. Assert that all tests pass, confirming that broken imports and paths have been resolved.

# Subtasks:
## 1. Consolidate Project Dependencies into pyproject.toml [pending]
### Dependencies: None
### Description: Audit the entire codebase to identify all production and development dependencies, and consolidate them into a single `pyproject.toml` file, deprecating any existing `requirements.txt` files.
### Details:
Identify all required libraries, including `packaging`, `ccxt`, `ccxt.pro`, `textual`, `pytest`, `pytest-benchmark`, and `pytest-asyncio`. Add production libraries to `[project.dependencies]` and development/testing libraries to `[project.optional-dependencies.test]`.

## 2. Create Automated Development Environment Setup Script [pending]
### Dependencies: 6.1
### Description: Develop a script to automate the creation of a virtual environment and the installation of all project and test dependencies from the `pyproject.toml` file.
### Details:
Create a `Makefile` or a shell script (e.g., `scripts/setup-dev.sh`) that automates the setup process. The script should execute `pip install -e .[test]`. Ensure the script is executable and document its usage in the `README.md`.

## 3. Refactor and Fix Broken Test Suite Imports [pending]
### Dependencies: 6.2
### Description: Systematically review and repair the existing test suite under the `tests/` directory by fixing all broken import statements and incorrect file paths resulting from recent code refactoring.
### Details:
Go through each file in the `tests/` directory and correct all `import` statements. The goal is to make the entire test suite discoverable and runnable by `pytest` from the project root without any import-related errors.

## 4. Implement WebSocket Mocking Framework [pending]
### Dependencies: 6.3
### Description: Create a configurable and reusable WebSocket mocking framework to simulate real-time data streams for testing exchange adapters.
### Details:
Develop a mock server/client in `tests/mocks/websocket.py`. It must simulate the full WebSocket lifecycle: connection, authentication, subscription, data messages, heartbeats, and disconnections. It should be integrated with `pytest` fixtures for easy configuration in tests.

## 5. Establish Performance Benchmarking Suite [pending]
### Dependencies: 6.3
### Description: Integrate the `pytest-benchmark` library and create an initial suite of performance tests for critical application components.
### Details:
Add and configure `pytest-benchmark`. Create a new test suite in `tests/performance/`. Implement initial benchmarks targeting data transformation logic within adapters and the event processing throughput of the `EventEngine`.

## 6. Set Up CI Workflow with Static Analysis [pending]
### Dependencies: 6.2
### Description: Create a foundational Continuous Integration (CI) pipeline using GitHub Actions that automatically performs linting and static code analysis.
### Details:
Create a `.github/workflows/ci.yml` file. The workflow must trigger on pushes and pull requests to `main`. It should include steps to check out code, set up Python, install dependencies via the setup script, and run static analysis tools like `flake8` and `black --check`.

## 7. Integrate Full Test Suite into CI Pipeline [pending]
### Dependencies: 6.3, 6.4, 6.5, 6.6
### Description: Enhance the CI pipeline to execute the complete test suite, including all unit, integration, and performance tests, to validate code changes.
### Details:
Add a new step to the `ci.yml` workflow that executes the full `pytest` suite. This step should run after the static analysis step has passed. The CI run's success or failure must depend on the test results.

